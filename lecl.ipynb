{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03785673",
   "metadata": {},
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbc649",
   "metadata": {},
   "source": [
    "## Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be869b6",
   "metadata": {},
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c40c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58ef665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm= init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aebec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fac354",
   "metadata": {},
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e2f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21838/2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1322ebca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '## Retrieval Augmented Generation: A Quick Overview\\n\\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of large language models (LLMs) by providing them with relevant external knowledge during the generation process. Instead of relying solely on the knowledge embedded within their training data, RAG allows LLMs to access and incorporate up-to-date and domain-specific information.\\n\\n**How it Works:**\\n\\nThe process generally involves these steps:\\n\\n1. **Retrieval:** Given a user query, a retrieval module (e.g., a vector database like Faiss or a search engine) identifies and retrieves relevant documents or knowledge snippets from a knowledge base. This knowledge base can be a collection of documents, web pages, database entries, or any other structured or unstructured data source.\\n2. **Augmentation:** The retrieved information is then combined with the original user query and fed into the LLM.\\n3. **Generation:** The LLM uses this augmented input to generate a more informed and contextually relevant response.\\n\\n**Benefits of RAG:**\\n\\n* **Improved Accuracy:**  LLMs can avoid generating inaccurate or outdated information by grounding their responses in external knowledge.\\n* **Reduced Hallucinations:** By relying on verifiable sources, RAG minimizes the risk of the LLM inventing facts or making unsupported claims.\\n* **Enhanced Domain Specificity:** RAG allows LLMs to perform well in niche domains by providing access to specialized knowledge bases.\\n* **Increased Transparency and Explainability:**  RAG can often provide citations or links to the retrieved sources, making it easier to verify the information and understand the reasoning behind the generated response.\\n* **Continuous Learning and Adaptability:** RAG systems can be easily updated with new information by updating the knowledge base, without requiring retraining of the LLM.\\n\\n**Challenges:**\\n\\n* **Retrieval Quality:** The effectiveness of RAG heavily relies on the accuracy and relevance of the retrieved information.  Poor retrieval can lead to irrelevant or misleading responses.\\n* **Knowledge Base Management:**  Maintaining and updating the knowledge base can be a complex and resource-intensive task.\\n* **Computational Cost:** Retrieval and processing of external knowledge can add to the computational cost of generation.\\n* **Context Window Limitations:** LLMs have limitations on the amount of text they can process at once. Efficiently managing the retrieved context within the context window is crucial.\\n\\n**In conclusion, Retrieval Augmented Generation is a promising approach for improving the performance and reliability of LLMs by integrating external knowledge. While challenges remain, RAG is rapidly evolving and becoming a key component in many applications that require accurate, up-to-date, and domain-specific information.**'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a695db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Retrieval Augmented Generation: A Quick Overview\n",
       "\n",
       "Retrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of large language models (LLMs) by providing them with relevant external knowledge during the generation process. Instead of relying solely on the knowledge embedded within their training data, RAG allows LLMs to access and incorporate up-to-date and domain-specific information.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "The process generally involves these steps:\n",
       "\n",
       "1. **Retrieval:** Given a user query, a retrieval module (e.g., a vector database like Faiss or a search engine) identifies and retrieves relevant documents or knowledge snippets from a knowledge base. This knowledge base can be a collection of documents, web pages, database entries, or any other structured or unstructured data source.\n",
       "2. **Augmentation:** The retrieved information is then combined with the original user query and fed into the LLM.\n",
       "3. **Generation:** The LLM uses this augmented input to generate a more informed and contextually relevant response.\n",
       "\n",
       "**Benefits of RAG:**\n",
       "\n",
       "* **Improved Accuracy:**  LLMs can avoid generating inaccurate or outdated information by grounding their responses in external knowledge.\n",
       "* **Reduced Hallucinations:** By relying on verifiable sources, RAG minimizes the risk of the LLM inventing facts or making unsupported claims.\n",
       "* **Enhanced Domain Specificity:** RAG allows LLMs to perform well in niche domains by providing access to specialized knowledge bases.\n",
       "* **Increased Transparency and Explainability:**  RAG can often provide citations or links to the retrieved sources, making it easier to verify the information and understand the reasoning behind the generated response.\n",
       "* **Continuous Learning and Adaptability:** RAG systems can be easily updated with new information by updating the knowledge base, without requiring retraining of the LLM.\n",
       "\n",
       "**Challenges:**\n",
       "\n",
       "* **Retrieval Quality:** The effectiveness of RAG heavily relies on the accuracy and relevance of the retrieved information.  Poor retrieval can lead to irrelevant or misleading responses.\n",
       "* **Knowledge Base Management:**  Maintaining and updating the knowledge base can be a complex and resource-intensive task.\n",
       "* **Computational Cost:** Retrieval and processing of external knowledge can add to the computational cost of generation.\n",
       "* **Context Window Limitations:** LLMs have limitations on the amount of text they can process at once. Efficiently managing the retrieved context within the context window is crucial.\n",
       "\n",
       "**In conclusion, Retrieval Augmented Generation is a promising approach for improving the performance and reliability of LLMs by integrating external knowledge. While challenges remain, RAG is rapidly evolving and becoming a key component in many applications that require accurate, up-to-date, and domain-specific information.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cf37d",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eebeae",
   "metadata": {},
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5480531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a19d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Retrieval Augmented Generation (RAG): A Quick Overview\n",
       "\n",
       "Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) like GPT-3 by allowing them to access and incorporate external knowledge sources before generating responses.  Instead of relying solely on the information they were trained on (which can be limited, outdated, or biased), RAG allows LLMs to \"look up\" relevant information and use it to inform their answers.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "The process typically involves these steps:\n",
       "\n",
       "1. **Retrieval:** Given a user query, the system first retrieves relevant documents or passages from an external knowledge base (e.g., a document database, website, or knowledge graph). This retrieval is typically done using techniques like semantic search, which aims to find documents that are semantically similar to the query, even if they don't share the exact same keywords.\n",
       "\n",
       "2. **Augmentation:** The retrieved information is then combined with the original user query to create an augmented prompt. This augmented prompt includes both the user's question and the relevant context retrieved from the knowledge base.\n",
       "\n",
       "3. **Generation:** The LLM then uses this augmented prompt to generate a response. By having access to the retrieved information, the LLM can provide more accurate, up-to-date, and contextually relevant answers.\n",
       "\n",
       "**Benefits of RAG:**\n",
       "\n",
       "* **Improved Accuracy and Relevance:** RAG allows LLMs to access and use external knowledge, leading to more accurate and relevant responses.\n",
       "* **Reduced Hallucinations:** By grounding responses in external knowledge, RAG helps to reduce the likelihood of LLMs generating incorrect or fabricated information.\n",
       "* **Increased Transparency and Explainability:** RAG can provide citations or links to the retrieved sources, making it easier to understand where the information came from and verify its accuracy.\n",
       "* **Adaptability to New Information:** RAG allows LLMs to adapt to new information without requiring retraining.  Simply update the knowledge base, and the LLM can immediately access and use the new information.\n",
       "* **Customization and Control:**  RAG allows for the tailoring of LLMs to specific domains or applications by selecting the appropriate knowledge base.\n",
       "\n",
       "**Challenges:**\n",
       "\n",
       "* **Retrieval Quality:** The performance of RAG depends heavily on the quality of the retrieval process.  Poor retrieval can lead to irrelevant or inaccurate information being used by the LLM.\n",
       "* **Complexity:**  Implementing RAG can be more complex than simply using an LLM directly. It requires setting up and maintaining a knowledge base and implementing a retrieval mechanism.\n",
       "* **Prompt Engineering:** Crafting effective prompts that incorporate the retrieved information can be challenging.\n",
       "* **Latency:**  Retrieving information from an external knowledge base can add latency to the response time.\n",
       "\n",
       "**In summary, RAG is a powerful technique that significantly enhances the capabilities of LLMs by allowing them to access and incorporate external knowledge. While it presents some challenges, the benefits of improved accuracy, relevance, and adaptability make it a valuable tool for a wide range of applications.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d088bf7",
   "metadata": {},
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9903fb",
   "metadata": {},
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3cf4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7d6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1258d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee10cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade234d2",
   "metadata": {},
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71410a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73efdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab8109c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de1d16",
   "metadata": {},
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bccd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41048d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b359f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Artificial Intelligence: A Brief Overview\n",
       "\n",
       "Artificial Intelligence (AI) is a broad field of computer science focused on creating machines that can perform tasks that typically require human intelligence. These tasks include learning, problem-solving, reasoning, perception, and natural language understanding.\n",
       "\n",
       "**Key Areas of AI:**\n",
       "\n",
       "* **Machine Learning (ML):**  Algorithms that allow computers to learn from data without explicit programming.  Examples include supervised learning (e.g., image classification), unsupervised learning (e.g., clustering), and reinforcement learning (e.g., training game-playing agents).\n",
       "* **Natural Language Processing (NLP):**  Enables computers to understand, interpret, and generate human language.  Applications include chatbots, machine translation, and sentiment analysis.\n",
       "* **Computer Vision:**  Allows computers to \"see\" and interpret images and videos.  Used in object detection, facial recognition, and autonomous driving.\n",
       "* **Robotics:**  Deals with the design, construction, operation, and application of robots, often incorporating AI for autonomous behavior.\n",
       "\n",
       "**Current Applications:**\n",
       "\n",
       "AI is rapidly transforming various industries:\n",
       "\n",
       "* **Healthcare:**  Diagnosis, drug discovery, personalized medicine.\n",
       "* **Finance:**  Fraud detection, algorithmic trading, risk management.\n",
       "* **Manufacturing:**  Automation, quality control, predictive maintenance.\n",
       "* **Transportation:**  Autonomous vehicles, traffic optimization.\n",
       "* **Customer Service:**  Chatbots, personalized recommendations.\n",
       "\n",
       "**Future Trends:**\n",
       "\n",
       "* **Increased automation:**  AI will likely automate more tasks across industries, impacting the job market.\n",
       "* **Explainable AI (XAI):**  Efforts to make AI decision-making more transparent and understandable.\n",
       "* **Edge AI:**  Processing AI algorithms locally on devices, reducing latency and improving privacy.\n",
       "* **Ethical considerations:**  Addressing biases in AI systems, ensuring fairness, and managing the potential for misuse.\n",
       "\n",
       "**In Conclusion:**\n",
       "\n",
       "AI is a powerful and rapidly evolving technology with the potential to significantly impact society. While offering numerous benefits, it also presents challenges that require careful consideration and responsible development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c0590",
   "metadata": {},
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e238fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77427722",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "500bbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf68e887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Retrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) by providing them with access to external knowledge. Instead of relying solely on the information encoded within their parameters during training, RAG allows LLMs to retrieve relevant information from a knowledge base (like a document store, database, or the web) before generating a response.\n",
       "**How it Works:**\n",
       "1. **Retrieval:** When a user asks a question, RAG first uses a retrieval mechanism (e.g., semantic search, keyword matching) to identify relevant documents or passages from the knowledge base.\n",
       "2. **Augmentation:** The retrieved information is then combined with the original user query. This augmented prompt is fed to the LLM.\n",
       "3. **Generation:** The LLM uses the augmented prompt, containing both the user's question and the retrieved context, to generate a more informed and accurate response.\n",
       "**Benefits of RAG:**\n",
       "* **Improved Accuracy:** By incorporating external knowledge, RAG reduces the likelihood of LLMs generating factually incorrect or hallucinated information.\n",
       "* **Up-to-Date Knowledge:** RAG allows LLMs to access and utilize current information, even if it wasn't part of their original training data.\n",
       "* **Explainability:** RAG makes it easier to understand why an LLM provided a particular answer, as the source of the information is often traceable.\n",
       "* **Customization:** RAG can be tailored to specific domains or knowledge bases, making LLMs more effective for specialized tasks.\n",
       "**Challenges:**\n",
       "* **Retrieval Quality:** The performance of RAG heavily depends on the accuracy and efficiency of the retrieval mechanism. Poor retrieval can lead to irrelevant or misleading information being used by the LLM.\n",
       "* **Context Window Limits:** LLMs have limited context windows, so efficiently incorporating retrieved information within these limits is crucial.\n",
       "* **Noise Reduction:** Retrieved documents may contain irrelevant or noisy information, which can negatively impact the LLM's generation process.\n",
       "**Applications:**\n",
       "RAG is used in a wide range of applications, including:\n",
       "* **Question Answering:** Providing accurate and comprehensive answers to user questions based on external knowledge.\n",
       "* **Chatbots:** Enhancing chatbot responses with relevant information from knowledge bases.\n",
       "* **Content Generation:** Creating more informed and engaging content by incorporating external data.\n",
       "* **Code Generation:** Generating code snippets that are tailored to specific libraries or frameworks.\n",
       "**In conclusion, RAG is a valuable technique for improving the performance and reliability of LLMs by enabling them to access and utilize external knowledge. While challenges remain, its benefits make it a crucial tool for building more intelligent and informative skynet systems.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da8500",
   "metadata": {},
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
