{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923403f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "chat_model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b83bd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73822426",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fb72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"EleutherAI/gpt-j-6b\"\n",
    "import os\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.5,\n",
    ")\n",
    "llm_chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57a364",
   "metadata": {},
   "source": [
    "# Few Shots prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c178083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    " \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3569d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_prompt,\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3cd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    {\"query\": lambda x : x[\"query\"],\n",
    "    \"context\": lambda x : x[\"context\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328bedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a501eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d76603",
   "metadata": {},
   "source": [
    "Then we define a list of examples with dictionaries containing the correct `input` and `output` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3174fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0c057",
   "metadata": {},
   "source": [
    "We then feed both of these into our `FewShotChatMessagePromptTemplate` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25e6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75b361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa004d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='# Aurelio AI Overview\\n\\nAurelio AI is an AI company that specializes in tooling for AI engineers, with a focus on language AI.\\n\\n*   **Expertise:** They have strong expertise in building AI agents and a strong background in information retrieval.\\n*   **Open Source Frameworks:** They are behind several open-source frameworks, including Semantic Router and Semantic Chunkers.\\n*   **AI Platform:** They offer an AI Platform that provides engineers with tools to help them build with AI.\\n*   **Development Services:** They provide development services to help other organizations bring their AI tech to market.\\n*   **LangChain Experts:** They became LangChain Experts in September 2024.\\n\\nIn summary, Aurelio AI offers a range of services and tools centered around language AI, including open-source frameworks, an AI platform, and development services, leveraging their expertise in AI agents, information retrieval, and the LangChain ecosystem.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--5dd0f30a-0bbd-4d64-83b5-01cae73d99f7-0' usage_metadata={'input_tokens': 207, 'output_tokens': 192, 'total_tokens': 399, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a861255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Aurelio AI Overview\n",
       "\n",
       "Aurelio AI is an AI company that specializes in tooling for AI engineers, with a focus on language AI.\n",
       "\n",
       "*   **Expertise:** They have strong expertise in building AI agents and a strong background in information retrieval.\n",
       "*   **Open Source Frameworks:** They are behind several open-source frameworks, including Semantic Router and Semantic Chunkers.\n",
       "*   **AI Platform:** They offer an AI Platform that provides engineers with tools to help them build with AI.\n",
       "*   **Development Services:** They provide development services to help other organizations bring their AI tech to market.\n",
       "*   **LangChain Experts:** They became LangChain Experts in September 2024.\n",
       "\n",
       "In summary, Aurelio AI offers a range of services and tools centered around language AI, including open-source frameworks, an AI platform, and development services, leveraging their expertise in AI agents, information retrieval, and the LangChain ecosystem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42978eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71dca4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdd733cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: ## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: ## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c40fd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c85b8367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Aurelio AI\n",
       "\n",
       "Aurelio AI is an AI company that focuses on providing tools and services for AI engineers, with a specialization in language AI.\n",
       "\n",
       "### Summary\n",
       "\n",
       "*   **Focus:** Language AI\n",
       "*   **Expertise:** Building AI agents and information retrieval\n",
       "*   **Open Source Frameworks:** Semantic Router and Semantic Chunkers\n",
       "*   **AI Platform:** Provides tooling for AI engineers.\n",
       "*   **Development Services:** Helps organizations bring their AI tech to market.\n",
       "*   **LangChain Experts:** Became LangChain Experts in September 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = prompt_template | chat_model\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context})\n",
    "display(Markdown(out.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057587f",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ca1e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9d7b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | chat_model\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query})\n",
    "print(no_cot_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cdaf73",
   "metadata": {},
   "source": [
    "The actual answer is `1392`, but the LLM _without_ CoT just hallucinates and gives us a guess. Now, we can add explicit CoT prompting to our system prompt to see if we can get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1572bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "687371b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this problem step-by-step:\n",
       "\n",
       "**1. Identify the Subproblems**\n",
       "\n",
       "We need to break this down by the number of digits in each number:\n",
       "\n",
       "*   **Subproblem 1:** Count the number of 1-digit numbers (1-9) and the keystrokes needed to type them.\n",
       "*   **Subproblem 2:** Count the number of 2-digit numbers (10-99) and the keystrokes needed to type them.\n",
       "*   **Subproblem 3:** Count the number of 3-digit numbers (100-500) and the keystrokes needed to type them.\n",
       "*   **Subproblem 4:** Calculate the total keystrokes.\n",
       "\n",
       "**2. Solve Subproblem 1: 1-digit numbers**\n",
       "\n",
       "*   Numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
       "*   Count: 9 numbers\n",
       "*   Keystrokes per number: 1\n",
       "*   Total keystrokes: 9 * 1 = 9\n",
       "\n",
       "**3. Solve Subproblem 2: 2-digit numbers**\n",
       "\n",
       "*   Numbers: 10 to 99\n",
       "*   Count: 99 - 10 + 1 = 90 numbers\n",
       "*   Keystrokes per number: 2\n",
       "*   Total keystrokes: 90 * 2 = 180\n",
       "\n",
       "**4. Solve Subproblem 3: 3-digit numbers**\n",
       "\n",
       "*   Numbers: 100 to 500\n",
       "*   Count: 500 - 100 + 1 = 401 numbers\n",
       "*   Keystrokes per number: 3\n",
       "*   Total keystrokes: 401 * 3 = 1203\n",
       "\n",
       "**5. Solve Subproblem 4: Calculate the total keystrokes**\n",
       "\n",
       "*   Total keystrokes = Keystrokes (1-digit) + Keystrokes (2-digit) + Keystrokes (3-digit)\n",
       "*   Total keystrokes = 9 + 180 + 1203 = 1392\n",
       "\n",
       "**Final Answer:** It takes 1392 keystrokes to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c3066",
   "metadata": {},
   "source": [
    "Now let's try without specifiying the use of cot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d811222",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9294750e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break this down:\n",
       "\n",
       "*   **1-Digit Numbers (1-9):** There are 9 numbers, each requiring 1 keystroke. Total: 9 * 1 = 9 keystrokes.\n",
       "\n",
       "*   **2-Digit Numbers (10-99):** There are 90 numbers, each requiring 2 keystrokes. Total: 90 * 2 = 180 keystrokes.\n",
       "\n",
       "*   **3-Digit Numbers (100-500):** There are 401 numbers, each requiring 3 keystrokes. Total: 401 * 3 = 1203 keystrokes.\n",
       "\n",
       "Now, add them up:\n",
       "\n",
       "9 + 180 + 1203 = 1392\n",
       "\n",
       "Therefore, it takes **1392** keystrokes to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
