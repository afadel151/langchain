{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f88814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm= init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57300f9f",
   "metadata": {},
   "source": [
    "## 1. `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae5a63",
   "metadata": {},
   "source": [
    "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6551246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6242/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c244b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52239e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa48259",
   "metadata": {},
   "source": [
    "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250a0637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fcc60",
   "metadata": {},
   "source": [
    "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b40b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6242/2101025848.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "##  deprecated: use RunnableWithMessageHistory instead\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72e2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James!'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319427e",
   "metadata": {},
   "source": [
    "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
    "\n",
    "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fc6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Akram.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c41a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af777a15",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48c7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfdd8c",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4fd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467eb1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James, nice to meet you! My name is Akram. How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d7a66355-c380-424b-8c52-77e3064ca8bc-0', usage_metadata={'input_tokens': 15, 'output_tokens': 22, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e1c4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3bc69686-0488-45ed-a356-e7bee7af0361-0', usage_metadata={'input_tokens': 42, 'output_tokens': 6, 'total_tokens': 48, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0deee",
   "metadata": {},
   "source": [
    "## 2. `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a70c04",
   "metadata": {},
   "source": [
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
    "\n",
    "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
    "\n",
    "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c00da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6242/3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9a5e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f17f36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f024adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'I am sorry, I do not know your name. I have no access to any personal information.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5095e",
   "metadata": {},
   "source": [
    "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f4628eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory,BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k : int = Field(default_factory=int)\n",
    "    \n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(\"BufferWindowMessageHistory initialized with k =\", k)\n",
    "    \n",
    "    def add_messages(self,messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f49da6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cfdcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c96eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "BufferWindowMessageHistory initialized with k = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James, nice to meet you. My name is Akram. How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fe804c1b-0f5b-449b-82ff-cfa3ec6eb1b2-0', usage_metadata={'input_tokens': 15, 'output_tokens': 22, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48f52a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91a966c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to personal information, so I don't know your name. You haven't told me yet!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--82ef1e17-f4a7-4b2e-b36f-89278695cc11-0', usage_metadata={'input_tokens': 47, 'output_tokens': 28, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6797e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "BufferWindowMessageHistory initialized with k = 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James, nice to meet you! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--08b67b83-b6a3-484a-b91c-1d37305bbad7-0', usage_metadata={'input_tokens': 15, 'output_tokens': 16, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26f1613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f979e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I don't have access to personal information like your name. You haven't told me your name yet. If you'd like to share it, feel free!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--00c6983d-5507-4063-8861-84b47cc65180-0', usage_metadata={'input_tokens': 161, 'output_tokens': 42, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa87f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"As a large language model, I don't have access to personal information like your name. You haven't told me your name yet. If you'd like to share it, feel free!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--00c6983d-5507-4063-8861-84b47cc65180-0', usage_metadata={'input_tokens': 161, 'output_tokens': 42, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"As a large language model, I don't have access to personal information like your name. You haven't told me your name yet. If you'd like to share it, feel free!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--00c6983d-5507-4063-8861-84b47cc65180-0', usage_metadata={'input_tokens': 161, 'output_tokens': 42, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac751b",
   "metadata": {},
   "source": [
    "## 3. `ConversationSummaryMemory`\n",
    "\n",
    "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
    "\n",
    "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd2c15b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6242/988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46151215",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a00c9b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James.\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn't have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn't have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn't have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case. James states he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory again. The AI acknowledges James is revisiting these foundational memories and suggests considering how they can be used as building blocks or how their limitations necessitate more advanced architectures. It then offers five options for further discussion: a detailed comparison of the two, practical code examples, bridging to advanced architectures by combining them with other techniques, a deeper dive into their limitations, or a pivot to discussing alternative, more advanced memory architectures. The AI then asks James which direction would be most helpful.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn't have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case. James states he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory again. The AI acknowledges James is revisiting these foundational memories and suggests considering how they can be used as building blocks or how their limitations necessitate more advanced architectures. It then offers five options for further discussion: a detailed comparison of the two, practical code examples, bridging to advanced architectures by combining them with other techniques, a deeper dive into their limitations, or a pivot to discussing alternative, more advanced memory architectures. The AI then asks James which direction would be most helpful. James states that buffer memory just stores the entire conversation. The AI confirms this, specifying that ConversationBufferMemory stores the entire conversation history as a single string or list, offering complete recall, but suffering from scalability issues due to the growing context length exceeding the language model's input size, and context dilution as earlier turns become less relevant. The AI offers to elaborate on scalability or context dilution, provide examples, or discuss how ConversationBufferWindowMemory addresses these issues.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'The human introduces himself as James. The AI responds, stating it doesn\\'t have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James\\'s understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different \\'k\\' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn\\'t have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn\\'t have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case. James states he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory again. The AI acknowledges James is revisiting these foundational memories and suggests considering how they can be used as building blocks or how their limitations necessitate more advanced architectures. It then offers five options for further discussion: a detailed comparison of the two, practical code examples, bridging to advanced architectures by combining them with other techniques, a deeper dive into their limitations, or a pivot to discussing alternative, more advanced memory architectures. The AI then asks James which direction would be most helpful. James states that buffer memory just stores the entire conversation. The AI confirms this, specifying that ConversationBufferMemory stores the entire conversation history as a single string or list, offering complete recall, but suffering from scalability issues due to the growing context length exceeding the language model\\'s input size, and context dilution as earlier turns become less relevant. The AI offers to elaborate on scalability or context dilution, provide examples, or discuss how ConversationBufferWindowMemory addresses these issues.',\n",
       " 'response': \"That's absolutely correct, James. ConversationBufferWindowMemory maintains a queue or list that holds only the *k* most recent messages (or turns) in the conversation. Once the queue is full (it contains *k* messages), adding a new message causes the oldest message to be dropped from the queue, following a First-In, First-Out (FIFO) principle.\\n\\nTo be even more precise, here's a slightly more detailed breakdown:\\n\\n*   **FIFO Behavior:** As you mentioned, it operates on a FIFO basis. The oldest interaction is discarded to make room for the newest one. This ensures that the most recent interactions are always prioritized.\\n\\n*   **Fixed Size:** The key characteristic is its fixed size. The *k* value is crucial and needs to be chosen carefully based on the expected length and complexity of the conversation.\\n\\n*   **Trade-off:** It offers a trade-off between retaining conversational context and managing computational costs. By limiting the memory to a fixed size, it avoids the scalability issues of ConversationBufferMemory but at the cost of potentially losing relevant information from earlier in the conversation.\\n\\n*   **Suitable Use Cases:** It is well-suited for scenarios where only the most recent context is important, such as:\\n\\n    *   Simple question-answering tasks where the answer depends only on the immediate question.\\n    *   Task-oriented dialogues where the current goal is the primary focus.\\n    *   Short conversations where the overall length remains within reasonable bounds.\\n\\n*   **Limitations:** Its key limitation is its inability to handle long-range dependencies or complex reasoning that requires information from earlier in the conversation. If a piece of information is dropped from the window, the model loses access to it.\\n\\nWould you like me to elaborate on any of these aspects, such as:\\n\\n*   The impact of choosing different values for *k*?\\n*   How ConversationBufferWindowMemory can be combined with other memory mechanisms to mitigate its limitations?\\n*   Practical code examples of how to implement and use ConversationBufferWindowMemory?\\n*   A more in-depth comparison of its limitations compared to more advanced memory architectures?\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6cab68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI responds, stating it doesn't have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn't have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn't have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case. James states he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory again. The AI acknowledges James is revisiting these foundational memories and suggests considering how they can be used as building blocks or how their limitations necessitate more advanced architectures. It then offers five options for further discussion: a detailed comparison of the two, practical code examples, bridging to advanced architectures by combining them with other techniques, a deeper dive into their limitations, or a pivot to discussing alternative, more advanced memory architectures. The AI then asks James which direction would be most helpful. James states that buffer memory just stores the entire conversation. The AI confirms this, specifying that ConversationBufferMemory stores the entire conversation history as a single string or list, offering complete recall, but suffering from scalability issues due to the growing context length exceeding the language model's input size, and context dilution as earlier turns become less relevant. The AI offers to elaborate on scalability or context dilution, provide examples, or discuss how ConversationBufferWindowMemory addresses these issues. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James's understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, fixed size determined by 'k', trade-off between context and scalability, suitability for simple question answering and short-term task-oriented dialogues, and limitations with long-range dependencies and complex reasoning. The AI offers to discuss the impact of different 'k' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'The human introduces himself as James. The AI responds, stating it doesn\\'t have a name in the human sense and can be called AI or \"the AI\". It identifies itself as a large language model trained by Google, designed to be informative and comprehensive. The AI explains its training on massive text data allows it to communicate and generate human-like text, and that it is still under development, but capable of many tasks. It then asks James what it can do for him. James is researching conversational memory. The AI confirms that conversational memory is a fascinating area, especially regarding the architectures used to implement it and the different approaches to utilizing it for conversational goals. It distinguishes between explicit and implicit memory, then lists memory networks, transformer-based approaches with long context windows, retrieval-augmented generation, hierarchical recurrent encoder-decoder models, and key-value memory networks as architectural examples. It then lists entity tracking, topic tracking, personalization, contextual understanding, and maintaining consistency as examples of how conversational memory is used. Finally, it asks James which of these areas resonates most with him so it can provide more targeted information and potentially point him to specific research papers. James says he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI responds that those are excellent choices and foundational components. It breaks down ConversationBufferMemory and ConversationBufferWindowMemory, explaining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation but suffers from scalability and context dilution. ConversationBufferWindowMemory maintains a fixed-size window of recent turns, improving scalability but losing older information. Both are best suited for short conversations. The AI suggests exploring more sophisticated memory mechanisms for complex scenarios and offers to answer specific questions about the two memory types or compare them to more advanced ones. James confirms that buffer memory stores the entire conversation. The AI elaborates on ConversationBufferMemory, explaining it concatenates each new turn to a single string or list, offering complete recall but suffering from scalability and context dilution issues as conversations grow longer, making it best suited for short conversations. The AI offers to elaborate on context dilution or scalability issues, or provide practical examples. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James\\'s understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, trade-off between context and scalability, the importance of choosing the window size (k), suitable use cases like simple question answering and short-term task-oriented dialogues, and limitations like struggling with long-range dependencies and complex reasoning. The AI then offers to discuss the impact of different \\'k\\' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures. James asks the AI what his name is. The AI responds that it doesn\\'t have access to personal information unless explicitly shared, and that James introduced himself as James. Therefore, his name is James. James re-introduces himself. The AI acknowledges James, restates it doesn\\'t have a name but can be called AI or \"the AI\", and asks how it can help him, suggesting he might have more questions about conversational memory. James states he is researching the different types of conversational memory. The AI acknowledges that James is diving deeper into conversational memory architectures and offers to categorize them by architectural type (simple buffer-based, vector embeddings & similarity search, key-value memory networks, graph-based memory, transformer-based memory, and retrieval-augmented generation) or by functionality/use case (entity tracking, topic tracking, personalization, fact retrieval, coherence & consistency, and goal tracking). It then asks James which area he is most interested in, or if he has any specific questions about a particular architecture or use case. James states he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory again. The AI acknowledges James is revisiting these foundational memories and suggests considering how they can be used as building blocks or how their limitations necessitate more advanced architectures. It then offers five options for further discussion: a detailed comparison of the two, practical code examples, bridging to advanced architectures by combining them with other techniques, a deeper dive into their limitations, or a pivot to discussing alternative, more advanced memory architectures. The AI then asks James which direction would be most helpful. James states that buffer memory just stores the entire conversation. The AI confirms this, specifying that ConversationBufferMemory stores the entire conversation history as a single string or list, offering complete recall, but suffering from scalability issues due to the growing context length exceeding the language model\\'s input size, and context dilution as earlier turns become less relevant. The AI offers to elaborate on scalability or context dilution, provide examples, or discuss how ConversationBufferWindowMemory addresses these issues. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms James\\'s understanding of ConversationBufferWindowMemory, elaborating on its FIFO behavior, fixed size determined by \\'k\\', trade-off between context and scalability, suitability for simple question answering and short-term task-oriented dialogues, and limitations with long-range dependencies and complex reasoning. The AI offers to discuss the impact of different \\'k\\' values, combining it with other mechanisms, code examples, or its limitations compared to more advanced architectures.',\n",
       " 'response': \"You introduced yourself as James. So, your name is James. Is there anything else I can help you with regarding conversational memory? Perhaps you'd like to explore the impact of different 'k' values in `ConversationBufferWindowMemory`, or see some code examples?\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd404c",
   "metadata": {},
   "source": [
    "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6d1fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class  ConversationSummaryMessageHistory(BaseChatMessageHistory,BaseModel):\n",
    "    messages : list[BaseMessage]  = Field(default_factory=list)\n",
    "    llm : ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
    "    \n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
    "        super().__init__(llm=llm)\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        summary_prompt= ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                 \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing summary: {summary}\\nNew messages:\\n {messages}\"\n",
    "            )\n",
    "        ])\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                summary=self.messages,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "747593c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15c94b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatGoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5476c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you. My name is Akram. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d5093e77-9fef-4dba-af8f-a4422f5c5150-0', usage_metadata={'input_tokens': 15, 'output_tokens': 25, 'total_tokens': 40, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9ac6972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='James introduced himself, and Akram responded, introducing himself and offering assistance.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db3bdd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"James and Akram introduced themselves. James is researching conversational memory, and Akram offered to help, asking James to specify what aspects he's most interested in, such as architectures, techniques, challenges, applications, or comparisons.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f479f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
